# src/inference.py
import re
import os
import torch
from transformers import AutoTokenizer, T5GemmaForConditionalGeneration
from peft import PeftModel, PeftConfig
from src.config import Config


class QAGenerator:
    def __init__(self, model_path, merge_weights=False):
        """
        Kh·ªüi t·∫°o QA Generator.
        
        Args:
            model_path: ƒê∆∞·ªùng d·∫´n t·ªõi model (LoRA adapter ho·∫∑c full model)
            merge_weights: N·∫øu True, merge LoRA weights v√†o base model (nhanh h∆°n khi inference)
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, self.tokenizer = self._load_model(model_path, merge_weights)
        self.model.eval()
        print(f"‚úÖ Model loaded on: {self.device}")

    def _load_model(self, model_path, merge_weights):
        """Load model - t·ª± ƒë·ªông detect PEFT ho·∫∑c full model"""
        
        # Ki·ªÉm tra xem c√≥ ph·∫£i PEFT model kh√¥ng
        is_peft = os.path.exists(os.path.join(model_path, "adapter_config.json"))
        
        if is_peft:
            print("üîß ƒêang load PEFT/LoRA model...")
            
            # Load PEFT config ƒë·ªÉ l·∫•y base model name
            peft_config = PeftConfig.from_pretrained(model_path)
            base_model_name = peft_config.base_model_name_or_path
            
            # Load tokenizer
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            
            # Load base model
            base_model = T5GemmaForConditionalGeneration.from_pretrained(
                base_model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            
            # Load LoRA adapter
            model = PeftModel.from_pretrained(base_model, model_path)
            
            # Merge weights n·∫øu c·∫ßn (inference nhanh h∆°n)
            if merge_weights:
                print("üîÄ ƒêang merge LoRA weights...")
                model = model.merge_and_unload()
                print("‚úÖ ƒê√£ merge weights th√†nh c√¥ng!")
            
        else:
            print("üì¶ ƒêang load Full model...")
            
            # Load tokenizer
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            
            # Load full model
            model = T5GemmaForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto"
            )
        
        return model, tokenizer

    def parse_output(self, text_output):
        """
        Chuy·ªÉn ƒë·ªïi chu·ªói text raw th√†nh list json mong mu·ªën
        Input: "question: Ai l√† T√¥? answer: PVƒê [SEP] question: NƒÉm n√†o? answer: 1987"
        Output: [{'question': 'Ai l√† T√¥?', 'answers': 'PVƒê'}, ...]
        """
        qa_list = []
        
        # 1. T√°ch c√°c c·∫∑p b·∫±ng [SEP]
        pairs = text_output.split(Config.PAIR_SEP.strip())
        
        for pair in pairs:
            pair = pair.strip()
            if not pair:
                continue
                
            # 2. D√πng Regex ho·∫∑c find ƒë·ªÉ t√°ch question v√† answer
            try:
                # T√¨m v·ªã tr√≠ c·ªßa tag answer
                a_idx = pair.find(Config.A_TAG.strip())
                q_idx = pair.find(Config.Q_TAG.strip())
                
                if a_idx != -1 and q_idx != -1:
                    # C·∫Øt chu·ªói
                    q_text = pair[q_idx + len(Config.Q_TAG.strip()): a_idx].strip()
                    a_text = pair[a_idx + len(Config.A_TAG.strip()):].strip()
                    
                    if q_text and a_text:  # Ch·ªâ th√™m n·∫øu c·∫£ 2 kh√¥ng r·ªóng
                        qa_list.append({
                            "question": q_text,
                            "answers": a_text
                        })
            except Exception as e:
                continue
                
        return qa_list

    def generate(self, context, num_beams=4, max_length=None, num_return_sequences=1):
        """
        Generate c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi t·ª´ context.
        
        Args:
            context: ƒêo·∫°n vƒÉn b·∫£n ƒë·∫ßu v√†o
            num_beams: S·ªë beams cho beam search
            max_length: ƒê·ªô d√†i t·ªëi ƒëa output
            num_return_sequences: S·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ
            
        Returns:
            List c√°c c·∫∑p Q&A d·∫°ng dict
        """
        if max_length is None:
            max_length = Config.MAX_TARGET_LENGTH
            
        input_text = Config.QA_PREFIX + context
        
        inputs = self.tokenizer(
            input_text, 
            max_length=Config.MAX_SOURCE_LENGTH, 
            truncation=True, 
            return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                num_beams=num_beams,
                num_return_sequences=num_return_sequences,
                early_stopping=True,
                do_sample=False,
            )
        
        # Decode v√† parse
        if num_return_sequences == 1:
            decoded_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return self.parse_output(decoded_text)
        else:
            # Tr·∫£ v·ªÅ nhi·ªÅu k·∫øt qu·∫£
            results = []
            for output in outputs:
                decoded_text = self.tokenizer.decode(output, skip_special_tokens=True)
                results.append(self.parse_output(decoded_text))
            return results

    def generate_with_sampling(self, context, temperature=0.7, top_p=0.9, top_k=50):
        """
        Generate v·ªõi sampling ƒë·ªÉ t·∫°o ƒëa d·∫°ng h∆°n.
        """
        input_text = Config.QA_PREFIX + context
        
        inputs = self.tokenizer(
            input_text, 
            max_length=Config.MAX_SOURCE_LENGTH, 
            truncation=True, 
            return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=Config.MAX_TARGET_LENGTH,
                do_sample=True,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
            )
        
        decoded_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return self.parse_output(decoded_text)

    def batch_generate(self, contexts, batch_size=8, num_beams=4):
        """
        Generate cho nhi·ªÅu contexts c√πng l√∫c.
        """
        all_results = []
        
        for i in range(0, len(contexts), batch_size):
            batch_contexts = contexts[i:i + batch_size]
            batch_inputs = [Config.QA_PREFIX + ctx for ctx in batch_contexts]
            
            inputs = self.tokenizer(
                batch_inputs,
                max_length=Config.MAX_SOURCE_LENGTH,
                truncation=True,
                padding=True,
                return_tensors="pt"
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=Config.MAX_TARGET_LENGTH,
                    num_beams=num_beams,
                    early_stopping=True,
                )
            
            for output in outputs:
                decoded_text = self.tokenizer.decode(output, skip_special_tokens=True)
                all_results.append(self.parse_output(decoded_text))
        
        return all_results


def main():
    import json
    
    # ============== C·∫§U H√åNH ==============
    # Ch·ªçn 1 trong 2 ƒë∆∞·ªùng d·∫´n:
    
    # Option 1: Load LoRA adapter
    model_path = "./results/lora_adapter"
    
    # Option 2: Load full model (n·∫øu kh√¥ng d√πng PEFT)
    # model_path = "./results/final_model"
    
    # Option 3: Load checkpoint c·ª• th·ªÉ
    # model_path = "./results/checkpoint-500"
    
    # ============== KH·ªûI T·∫†O ==============
    generator = QAGenerator(
        model_path=model_path,
        merge_weights=True  # True ƒë·ªÉ inference nhanh h∆°n
    )
    
    # ============== TEST ƒê∆†N ==============
    sample_context = """
    Ph·∫°m VƒÉn ƒê·ªìng (1 th√°ng 3 nƒÉm 1906 ‚Äì 29 th√°ng 4 nƒÉm 2000) l√† Th·ªß t∆∞·ªõng ƒë·∫ßu ti√™n 
    c·ªßa n∆∞·ªõc C·ªông h√≤a X√£ h·ªôi ch·ªß nghƒ©a Vi·ªát Nam t·ª´ nƒÉm 1976. √îng c√≥ t√™n g·ªçi th√¢n m·∫≠t l√† T√¥.
    """
    
    print("=" * 50)
    print("üìù Context:")
    print(sample_context.strip())
    print("=" * 50)
    
    # Generate Q&A
    result_json = generator.generate(sample_context)
    
    print("\nüéØ Generated Q&A:")
    print(json.dumps(result_json, ensure_ascii=False, indent=4))
    
    # ============== TEST BATCH ==============
    print("\n" + "=" * 50)
    print("üìö Batch Generation Test:")
    
    contexts = [
        "Python l√† ng√¥n ng·ªØ l·∫≠p tr√¨nh ƒë∆∞·ª£c t·∫°o b·ªüi Guido van Rossum v√†o nƒÉm 1991.",
        "H√† N·ªôi l√† th·ªß ƒë√¥ c·ªßa Vi·ªát Nam v·ªõi h∆°n 8 tri·ªáu d√¢n.",
    ]
    
    batch_results = generator.batch_generate(contexts)
    for i, (ctx, res) in enumerate(zip(contexts, batch_results)):
        print(f"\n--- Context {i+1} ---")
        print(f"Input: {ctx[:50]}...")
        print(f"Output: {json.dumps(res, ensure_ascii=False)}")


if __name__ == "__main__":
    main()