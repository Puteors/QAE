# train.py
import sys
import os
import torch
from transformers import (
    AutoTokenizer, 
    T5GemmaForConditionalGeneration, 
    Seq2SeqTrainingArguments, 
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    TaskType,
    get_peft_model,
    prepare_model_for_kbit_training
)
from src.config import Config
from src.dataset import QAGenDataset
from src.metrics import compute_metrics
from functools import partial


def print_trainable_parameters(model):
    """In s·ªë l∆∞·ª£ng tham s·ªë c√≥ th·ªÉ hu·∫•n luy·ªán"""
    trainable_params = 0
    all_params = 0
    for _, param in model.named_parameters():
        all_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"üìä Trainable params: {trainable_params:,} || "
        f"All params: {all_params:,} || "
        f"Trainable%: {100 * trainable_params / all_params:.2f}%"
    )


def load_model_and_tokenizer():
    """Load tokenizer v√† model v·ªõi PEFT config"""
    
    # 1. Load Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)
    
    # --- [FIX 1] X·ª≠ l√Ω PAD TOKEN (B·∫Øt bu·ªôc ƒë·ªÉ tr√°nh l·ªói indices) ---
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        print("‚ÑπÔ∏è ƒê√£ g√°n pad_token = eos_token")
    
    # C·∫≠p nh·∫≠t padding side (T5 th∆∞·ªùng l√† right ho·∫∑c m·∫∑c ƒë·ªãnh)
    tokenizer.padding_side = "right" 
    # -----------------------------------------------------------
    
    # 2. C·∫•u h√¨nh Quantization
    bnb_config = None
    if Config.USE_4BIT:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=Config.BNB_4BIT_QUANT_TYPE,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )
        print("‚úÖ ƒêang s·ª≠ d·ª•ng 4-bit Quantization (QLoRA)")
    
    # --- [FIX 2] X·ª≠ l√Ω Device Map ---
    # N·∫øu d√πng 4-bit: Bu·ªôc ph·∫£i d√πng device_map, nh∆∞ng ta √©p v·ªÅ GPU 0 ƒë·ªÉ tr√°nh l·ªói split
    # N·∫øu KH√îNG d√πng 4-bit: B·ªè device_map ƒë·ªÉ Trainer t·ª± qu·∫£n l√Ω (·ªïn ƒë·ªãnh nh·∫•t)
    
    device_map_config = None
    if Config.USE_4BIT:
        device_map_config = {"": 0} # √âp to√†n b·ªô model v√†o GPU 0
    else:
        device_map_config = None    # ƒê·ªÉ None, Trainer s·∫Ω t·ª± move model v√†o cuda:0
        
    print(f"‚öôÔ∏è Device Map config: {device_map_config}")

    # 3. Load Model
    model = T5GemmaForConditionalGeneration.from_pretrained(
        Config.MODEL_NAME,
        quantization_config=bnb_config,
        torch_dtype=torch.float16, # Lu√¥n d√πng fp16 cho nh·∫π
        device_map=device_map_config 
    )

    # ƒê·ªìng b·ªô config model v·ªõi tokenizer
    model.config.pad_token_id = tokenizer.pad_token_id
    model.config.eos_token_id = tokenizer.eos_token_id

    # 4. √Åp d·ª•ng PEFT/LoRA (n·∫øu b·∫≠t)
    if Config.USE_PEFT:
        print("=" * 50)
        print("üîß ƒêang c·∫•u h√¨nh PEFT/LoRA...")
        
        # Chu·∫©n b·ªã model cho k-bit training
        if Config.USE_4BIT:
            model = prepare_model_for_kbit_training(model)
        
        # C·∫•u h√¨nh LoRA
        lora_config = LoraConfig(
            task_type=TaskType.SEQ_2_SEQ_LM,
            r=Config.LORA_R,
            lora_alpha=Config.LORA_ALPHA,
            lora_dropout=Config.LORA_DROPOUT,
            target_modules=Config.LORA_TARGET_MODULES,
            bias="none",
            inference_mode=False,
        )
        
        # √Åp d·ª•ng LoRA
        model = get_peft_model(model, lora_config)
        
        # [FIX 3] ƒê·∫£m b·∫£o model in/out ƒë√∫ng ki·ªÉu d·ªØ li·ªáu khi d√πng Trainer
        # Gi√∫p tr√°nh l·ªói tensor mismatch
        for param in model.parameters():
            if param.requires_grad:
                param.data = param.data.to(torch.float32) # Cast layer ƒëang train v·ªÅ float32 ƒë·ªÉ ·ªïn ƒë·ªãnh
                
        print_trainable_parameters(model)
        print("=" * 50)
    
    return model, tokenizer


def main():
    # 1. Load Model & Tokenizer
    print("üì• ƒêang t·∫£i model v√† tokenizer...")
    model, tokenizer = load_model_and_tokenizer()

    # 2. Prepare Datasets
    print("üìÇ ƒêang t·∫£i datasets...")
    train_dataset = QAGenDataset("./data/train.json", tokenizer)
    val_dataset = QAGenDataset("./data/validation.json", tokenizer)
    print(f"   Train samples: {len(train_dataset)}")
    print(f"   Val samples: {len(val_dataset)}")

    # 3. Data Collator
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer, 
        model=model
    )

    # 4. Training Arguments
    training_args = Seq2SeqTrainingArguments(
        output_dir=Config.OUTPUT_DIR,
        
        # --- EVALUATION & SAVING ---
        eval_strategy="steps",
        eval_steps=Config.EVAL_STEPS,
        save_strategy="steps",
        save_steps=Config.EVAL_STEPS,
        load_best_model_at_end=True,
        metric_for_best_model="rougeL",
        greater_is_better=True,
        save_total_limit=Config.SAVE_TOTAL_LIMIT,
        
        # --- TRAINING PARAMS ---
        learning_rate=Config.LEARNING_RATE,
        per_device_train_batch_size=Config.BATCH_SIZE,
        per_device_eval_batch_size=Config.BATCH_SIZE,
        weight_decay=0.01,
        num_train_epochs=Config.EPOCHS,
        warmup_ratio=0.1,              # Warmup 10% s·ªë steps
        
        # --- GENERATION ---
        predict_with_generate=True,
        generation_max_length=Config.MAX_TARGET_LENGTH,
        
        # --- OPTIMIZATION ---
        fp16=True,
        gradient_checkpointing=True,
        gradient_checkpointing_kwargs={"use_reentrant": False},
        
        # --- LOGGING ---
        logging_dir='./logs',
        logging_steps=50,
        report_to="tensorboard",
    )

    # 5. Initialize Trainer
    compute_metrics_func = partial(compute_metrics, tokenizer=tokenizer)

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        processing_class=tokenizer, 
        data_collator=data_collator,
        compute_metrics=compute_metrics_func
    )

    # 6. Train
    print("\nüöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán...")
    trainer.train()

    # 7. Save Model
    if Config.USE_PEFT:
        # L∆∞u LoRA adapter
        lora_save_path = os.path.join(Config.OUTPUT_DIR, "lora_adapter")
        model.save_pretrained(lora_save_path)
        tokenizer.save_pretrained(lora_save_path)
        print(f"‚úÖ ƒê√£ l∆∞u LoRA adapter t·∫°i: {lora_save_path}")
    else:
        # L∆∞u full model
        final_path = os.path.join(Config.OUTPUT_DIR, "final_model")
        trainer.save_model(final_path)
        tokenizer.save_pretrained(final_path)
        print(f"‚úÖ ƒê√£ l∆∞u model t·∫°i: {final_path}")


if __name__ == "__main__":
    main()