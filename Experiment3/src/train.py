# train.py
import sys
import os
import torch
from transformers import (
    AutoTokenizer, 
    T5GemmaForConditionalGeneration, 
    Seq2SeqTrainingArguments, 
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    TaskType,
    get_peft_model,
    prepare_model_for_kbit_training
)
from src.config import Config
from src.dataset import QAGenDataset
from src.metrics import compute_metrics
from functools import partial


def print_trainable_parameters(model):
    """In s·ªë l∆∞·ª£ng tham s·ªë c√≥ th·ªÉ hu·∫•n luy·ªán"""
    trainable_params = 0
    all_params = 0
    for _, param in model.named_parameters():
        all_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"üìä Trainable params: {trainable_params:,} || "
        f"All params: {all_params:,} || "
        f"Trainable%: {100 * trainable_params / all_params:.2f}%"
    )


def load_model_and_tokenizer():
    """Load tokenizer v√† model v·ªõi PEFT config"""
    
    # 1. Load Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)
    
    # 2. C·∫•u h√¨nh Quantization (n·∫øu b·∫≠t)
    bnb_config = None
    if Config.USE_4BIT:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=Config.BNB_4BIT_QUANT_TYPE,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )
        print("‚úÖ ƒêang s·ª≠ d·ª•ng 4-bit Quantization (QLoRA)")
    
    # 3. Load Model
    model = T5GemmaForConditionalGeneration.from_pretrained(
        Config.MODEL_NAME,
        quantization_config=bnb_config,
        torch_dtype=torch.float16 if not Config.USE_4BIT else None,
        device_map="auto"
    )
    
    # 4. √Åp d·ª•ng PEFT/LoRA (n·∫øu b·∫≠t)
    if Config.USE_PEFT:
        print("=" * 50)
        print("üîß ƒêang c·∫•u h√¨nh PEFT/LoRA...")
        
        # Chu·∫©n b·ªã model cho k-bit training (n·∫øu d√πng quantization)
        if Config.USE_4BIT:
            model = prepare_model_for_kbit_training(model)
        
        # C·∫•u h√¨nh LoRA
        lora_config = LoraConfig(
            task_type=TaskType.SEQ_2_SEQ_LM,
            r=Config.LORA_R,
            lora_alpha=Config.LORA_ALPHA,
            lora_dropout=Config.LORA_DROPOUT,
            target_modules=Config.LORA_TARGET_MODULES,
            bias="none",
            inference_mode=False,
        )
        
        # √Åp d·ª•ng LoRA
        model = get_peft_model(model, lora_config)
        
        print(f"   LoRA Rank (r): {Config.LORA_R}")
        print(f"   LoRA Alpha: {Config.LORA_ALPHA}")
        print(f"   LoRA Dropout: {Config.LORA_DROPOUT}")
        print(f"   Target Modules: {Config.LORA_TARGET_MODULES}")
        print_trainable_parameters(model)
        print("=" * 50)
    else:
        print("‚ö†Ô∏è PEFT ƒë√£ t·∫Øt - S·ª≠ d·ª•ng Full Fine-tuning")
    
    return model, tokenizer


def main():
    # 1. Load Model & Tokenizer
    print("üì• ƒêang t·∫£i model v√† tokenizer...")
    model, tokenizer = load_model_and_tokenizer()

    # 2. Prepare Datasets
    print("üìÇ ƒêang t·∫£i datasets...")
    train_dataset = QAGenDataset("./data/train.json", tokenizer)
    val_dataset = QAGenDataset("./data/validation.json", tokenizer)
    print(f"   Train samples: {len(train_dataset)}")
    print(f"   Val samples: {len(val_dataset)}")

    # 3. Data Collator
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer, 
        model=model
    )

    # 4. Training Arguments
    training_args = Seq2SeqTrainingArguments(
        output_dir=Config.OUTPUT_DIR,
        
        # --- EVALUATION & SAVING ---
        eval_strategy="steps",
        eval_steps=Config.EVAL_STEPS,
        save_strategy="steps",
        save_steps=Config.EVAL_STEPS,
        load_best_model_at_end=True,
        metric_for_best_model="rougeL",
        greater_is_better=True,
        save_total_limit=Config.SAVE_TOTAL_LIMIT,
        
        # --- TRAINING PARAMS ---
        learning_rate=Config.LEARNING_RATE,
        per_device_train_batch_size=Config.BATCH_SIZE,
        per_device_eval_batch_size=Config.BATCH_SIZE,
        weight_decay=0.01,
        num_train_epochs=Config.EPOCHS,
        warmup_ratio=0.1,              # Warmup 10% s·ªë steps
        
        # --- GENERATION ---
        predict_with_generate=True,
        generation_max_length=Config.MAX_TARGET_LENGTH,
        
        # --- OPTIMIZATION ---
        fp16=True,
        gradient_checkpointing=True,
        gradient_checkpointing_kwargs={"use_reentrant": False},
        
        # --- LOGGING ---
        logging_dir='./logs',
        logging_steps=50,
        report_to="tensorboard",
    )

    # 5. Initialize Trainer
    compute_metrics_func = partial(compute_metrics, tokenizer=tokenizer)

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        processing_class=tokenizer, 
        data_collator=data_collator,
        compute_metrics=compute_metrics_func
    )

    # 6. Train
    print("\nüöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán...")
    trainer.train()

    # 7. Save Model
    if Config.USE_PEFT:
        # L∆∞u LoRA adapter
        lora_save_path = os.path.join(Config.OUTPUT_DIR, "lora_adapter")
        model.save_pretrained(lora_save_path)
        tokenizer.save_pretrained(lora_save_path)
        print(f"‚úÖ ƒê√£ l∆∞u LoRA adapter t·∫°i: {lora_save_path}")
    else:
        # L∆∞u full model
        final_path = os.path.join(Config.OUTPUT_DIR, "final_model")
        trainer.save_model(final_path)
        tokenizer.save_pretrained(final_path)
        print(f"‚úÖ ƒê√£ l∆∞u model t·∫°i: {final_path}")


if __name__ == "__main__":
    main()